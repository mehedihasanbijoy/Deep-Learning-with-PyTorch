{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIPmSvQj1LHn",
        "outputId": "a7ef0d21-8ab5-4ac3-be0c-0947695047d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dFXEOPv81Jay"
      },
      "outputs": [],
      "source": [
        "import einops\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OE4INbx1LJ2",
        "outputId": "0c2d1f7e-451e-4787-cdf6-043407820fc7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patch_size = 16\n",
        "latent_size = 768\n",
        "n_channels = 3\n",
        "num_heads = 12\n",
        "num_encoders = 12\n",
        "dropout = 0.1\n",
        "num_classes = 10\n",
        "img_size = 224\n",
        "\n",
        "epochs = 30\n",
        "base_lr = 10e-3\n",
        "weight_decay = 0.03\n",
        "batch_size = 4"
      ],
      "metadata": {
        "id": "5bU3smSx1LMY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, patch_size=patch_size, n_channels=n_channels, latent_size=latent_size, batch_size=batch_size, device=device): \n",
        "        super(InputEmbedding, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.n_channels = n_channels\n",
        "        self.latent_size = latent_size\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.input_size = self.patch_size * self.patch_size * self.n_channels\n",
        "\n",
        "        self.linear_projection = nn.Linear(self.input_size, self.latent_size) \n",
        "\n",
        "        self.class_token = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device) \n",
        "    \n",
        "    def forward(self, img):\n",
        "        img = img.to(self.device)\n",
        "        # print(img.shape)\n",
        "\n",
        "        # img to patches\n",
        "        patches = einops.rearrange(\n",
        "            img, 'b c (h h1) (w w1) -> b (h w) (h1 w1 c)', h1 = self.patch_size, w1 = self.patch_size\n",
        "        )\n",
        "        # print(patches.shape)\n",
        "\n",
        "        linear_projection = self.linear_projection(patches).to(self.device)\n",
        "        b, n, _ = linear_projection.shape\n",
        "        # print(linear_projection.shape)\n",
        "\n",
        "        linear_projection = torch.cat((self.class_token, linear_projection), dim=1)\n",
        "        pos_embed = einops.repeat(self.pos_embedding, 'b 1 d -> b m d', m = n+1)\n",
        "        # print(linear_projection.shape)\n",
        "\n",
        "        linear_projection += pos_embed\n",
        "\n",
        "        return linear_projection        "
      ],
      "metadata": {
        "id": "P-kg8XOC1LO_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = torch.randn((4, 3, 224, 224))\n",
        "test_class = InputEmbedding().to(device)\n",
        "embed_test = test_class(test_input)\n",
        "embed_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnASjKod1LRf",
        "outputId": "5476e311-c873-4392-b340-d348867797a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, latent_size=latent_size, num_heads=num_heads, dropout=dropout, device=device):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "\n",
        "        self.latent_size = latent_size\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout \n",
        "        self.device = device\n",
        "\n",
        "        self.norm = nn.LayerNorm(self.latent_size)\n",
        "\n",
        "        self.multihead = nn.MultiheadAttention(\n",
        "            self.latent_size, self.num_heads, dropout = self.dropout\n",
        "        )\n",
        "\n",
        "        self.enc_MLP = nn.Sequential(\n",
        "            nn.Linear(self.latent_size, self.latent_size*4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.latent_size*4, self.latent_size),\n",
        "            nn.Dropout(self.dropout)\n",
        "        )\n",
        "    \n",
        "    def forward(self, embedded_patches):\n",
        "        # first sublayer\n",
        "        first_norm_out = self.norm(embedded_patches)\n",
        "        attention_out = self.multihead(first_norm_out, first_norm_out, first_norm_out)[0]\n",
        "        # residual connection\n",
        "        first_added_out = attention_out + embedded_patches\n",
        "\n",
        "        # second sublayer\n",
        "        second_norm_out = self.norm(first_added_out)\n",
        "        # encMLP\n",
        "        encMLP_out = self.enc_MLP(second_norm_out)\n",
        "        # residual connection\n",
        "        second_added_out = encMLP_out + first_added_out\n",
        "\n",
        "        return second_added_out"
      ],
      "metadata": {
        "id": "EYvV_W6n1LT-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encoder = EncoderBlock().to(device)\n",
        "test_encoder_out = test_encoder(embed_test)\n",
        "test_encoder_out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdlUMgxq1LWm",
        "outputId": "636f5535-319e-41b7-975e-09623126eca0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, num_encoders=num_encoders, latent_size=latent_size, num_classes=num_classes, dropout=dropout, device=device):\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        self.num_encoders = num_encoders\n",
        "        self.latent_size = latent_size\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout = dropout \n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = InputEmbedding()\n",
        "\n",
        "        self.enc_stack = nn.ModuleList(\n",
        "            [EncoderBlock() for _ in range(self.num_encoders)]\n",
        "        )\n",
        "\n",
        "        self.MLP_head = nn.Sequential(\n",
        "            nn.LayerNorm(self.latent_size),\n",
        "            nn.Linear(self.latent_size, self.latent_size),\n",
        "            nn.Linear(self.latent_size, self.num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        enc_output = self.embedding(inp)\n",
        "\n",
        "        for enc_layer in self.enc_stack:\n",
        "            enc_output = enc_layer.forward(enc_output)\n",
        "        \n",
        "        cls_token_embedding = enc_output[:, 0]\n",
        "\n",
        "        pred = self.MLP_head(cls_token_embedding)\n",
        "\n",
        "        return pred"
      ],
      "metadata": {
        "id": "KXxlPV2-1LZJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViT().to(device)\n",
        "vit_output = model(test_input)\n",
        "print(test_input.shape)\n",
        "print(vit_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X00YzIWp1Lbw",
        "outputId": "79252687-678a-4595-9def-b6ee0d02804c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3, 224, 224])\n",
            "torch.Size([4, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OrCCFIr31Lhj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sroVq7pk1LkH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-8mQvSs91Lmv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2VsVnj4V1Lpa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSyvKEMi1Lr8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6JenK4F11LuX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ot32YLvq1Lw-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "szqiSpdb1Lzi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iRoiolcu1L1_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3bJ2Dc1S1L4f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rw4pOiRQ1L7C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_DEVfPAC1L9c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dwk8aWTL1L_z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fk2iV1Rm1MCg"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}
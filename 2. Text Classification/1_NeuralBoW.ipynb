{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Preprocess Text"
      ],
      "metadata": {
        "id": "1aNjMXqIYfMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install text-preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M4w_UW0tW0r",
        "outputId": "9f312622-5b20-48f0-adf4-c77c744ee9df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: text-preprocessing in /usr/local/lib/python3.8/dist-packages (0.1.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from text-preprocessing) (3.7)\n",
            "Requirement already satisfied: unittest-xml-reporting in /usr/local/lib/python3.8/dist-packages (from text-preprocessing) (3.2.0)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.8/dist-packages (from text-preprocessing) (0.1.73)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.8/dist-packages (from text-preprocessing) (0.7.1)\n",
            "Requirement already satisfied: names-dataset==2.1 in /usr/local/lib/python3.8/dist-packages (from text-preprocessing) (2.1.0)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.8/dist-packages (from contractions->text-preprocessing) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.8/dist-packages (from textsearch>=0.0.21->contractions->text-preprocessing) (0.3.1)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.8/dist-packages (from textsearch>=0.0.21->contractions->text-preprocessing) (1.4.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->text-preprocessing) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->text-preprocessing) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk->text-preprocessing) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->text-preprocessing) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from unittest-xml-reporting->text-preprocessing) (4.9.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from text_preprocessing import preprocess_text\n",
        "from text_preprocessing import to_lower, remove_email, remove_url, remove_punctuation\n",
        "\n",
        "preprocess_functions = [to_lower, remove_email, remove_url, remove_punctuation]\n",
        "\n",
        "def clean_text(text):\n",
        "    return preprocess_text(text, preprocess_functions)"
      ],
      "metadata": {
        "id": "EdoKVSFItYiT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c49942-47a7-4e11-dc6b-14a5f293f325"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment2label(sentiment):\n",
        "    return 0 if sentiment == 'negative' else 1"
      ],
      "metadata": {
        "id": "BGDzjCZGtXmr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/PyTorch/PyTorch-NLP-Tutorial/Corpus/IMDB Dataset.csv')\n",
        "df['text'] = df['review'].apply(clean_text)\n",
        "df['label'] = df['sentiment'].apply(sentiment2label)\n",
        "df = df[['text', 'label']]\n",
        "df = df.sample(frac=1)\n",
        "df.dropna(inplace=True) \n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "gezKwpMZtXuF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting into train and test set\n",
        "train_df = df.iloc[:int(len(df)*0.8), :].reset_index(drop=True)\n",
        "test_df = df.iloc[int(len(df)*0.8):, :].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "m0IuhYQ7fmZP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "obCp409AYppE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Vocabulary class to convert text into numerical values"
      ],
      "metadata": {
        "id": "z2uPXGV6YqSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary: \n",
        "    def __init__(self, freq_threshold=10, max_size=100000):\n",
        "        '''\n",
        "        freq_threshold : the minimum times a word must occur in corpus to be included in vocabulary\n",
        "        max_size : max vocab size\n",
        "        '''\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.max_size = max_size\n",
        "\n",
        "        self.itos = {0: '<PAD>', 1:'<SOS>', 2:'<EOS>', 3: '<UNK>'}\n",
        "        self.stoi = {k:j for j, k in self.itos.items()} \n",
        "          \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "    \n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return [tok.lower().strip() for tok in text.split(' ')]\n",
        "    \n",
        "    \n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        '''\n",
        "        build the vocabulary: create a dictionary mapping of index to string (itos) and string to index (stoi)\n",
        "        (itos) -> {5:'the', 6:'a', 7:'an'} | (stoi) -> {'the':5, 'a':6, 'an':7}\n",
        "        '''\n",
        "        frequencies = {} \n",
        "        idx = 4  # because 4 tokens already added -> (itos) -> {0: '<PAD>', 1:'<SOS>', 2:'<EOS>', 3: '<UNK>'}\n",
        "        \n",
        "        # calculate the freq of words\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer(sentence):\n",
        "                if word not in frequencies.keys():\n",
        "                    frequencies[word] = 1\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "                    \n",
        "                    \n",
        "        # limit vocab by removing low freq words\n",
        "        frequencies = {k:v for k,v in frequencies.items() if v > self.freq_threshold} \n",
        "        \n",
        "        # limit vocab to the max_size specified\n",
        "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx = 4 for pad, start, end , unk\n",
        "            \n",
        "        # create vocab\n",
        "        for word in frequencies.keys():\n",
        "            self.stoi[word] = idx\n",
        "            self.itos[idx] = word\n",
        "            idx += 1\n",
        "\n",
        "\n",
        "    # to convert text into numeric values\n",
        "    def numericalize(self, text):\n",
        "        '''\n",
        "        convert the list of words to a list of corresponding indexes\n",
        "        eg. cat and a dog -> [4, 5, 6, 3]\n",
        "        '''   \n",
        "        tokenized_text = self.tokenizer(text)  # tokenize text \n",
        "        numericalized_text = []\n",
        "\n",
        "        for token in tokenized_text:\n",
        "            if token in self.stoi.keys():\n",
        "                numericalized_text.append(self.stoi[token])\n",
        "            else: # out-of-vocab (OOV) words are represented by UNK token index\n",
        "                numericalized_text.append(self.stoi['<UNK>'])\n",
        "                \n",
        "        return numericalized_text"
      ],
      "metadata": {
        "id": "_nH53zlbtX1k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPwxAk9jxx_m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Dataloader"
      ],
      "metadata": {
        "id": "v0haTYc-ZSqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train DataLoader**"
      ],
      "metadata": {
        "id": "zjmJ6-w4ZaM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class TrainDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, text_column, label_column, freq_threshold=5, vocab_size=10000):\n",
        "        self.df = df\n",
        "        \n",
        "        # get texts and labels\n",
        "        self.texts = self.df[text_column]\n",
        "        self.labels = self.df[label_column]\n",
        "        \n",
        "        # build vocabulary\n",
        "        self.vocab = Vocabulary(freq_threshold, vocab_size)\n",
        "        self.vocab.build_vocabulary(self.texts.tolist())\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[int(index)]\n",
        "        label = self.labels[index]\n",
        "            \n",
        "        # numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
        "        numerialized_text = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numerialized_text += self.vocab.numericalize(text)\n",
        "        numerialized_text.append(self.vocab.stoi[\"<EOS>\"])\n",
        "    \n",
        "        label = [float(label)]\n",
        "        \n",
        "        return torch.tensor(numerialized_text), torch.tensor(label, requires_grad = True) "
      ],
      "metadata": {
        "id": "zOL_KhUUtX6m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TrainDataset(\n",
        "    df = train_df, text_column = 'text', label_column = 'label', \n",
        "    freq_threshold = 10, vocab_size = 25000\n",
        ")\n",
        "\n",
        "print(f'{df.loc[1]}\\n')\n",
        "\n",
        "text, label = train_dataset[1]\n",
        "print(text)\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVRZ1OFMxw5j",
        "outputId": "2ff79571-74ef-4317-e240-343a5273c478"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text     very much a film from the times  extremely lon...\n",
            "label                                                    0\n",
            "Name: 1, dtype: object\n",
            "\n",
            "tensor([    1,    56,    76,     5,    22,    39,     4,   216,    34,   555,\n",
            "          205,   834,    19,    59,   401,    86,  1947,     6,    37,   210,\n",
            "         1036,   906,   183,     4,   111,   369,    40,   943,    28,     5,\n",
            "          928,     7,   486,   300,   407,     6,  1948,    65,    17,   729,\n",
            "         5522,    18, 12006,    15,  7557,     9,  4341,  1935,    30,  2373,\n",
            "          462,    30,  2283,    76,     7,    27,    63, 20660,   145,     3,\n",
            "        14547, 15682,     9,   206,     5,  1786,     8,    68,    21,    30,\n",
            "          103,     9,  1862,     3,    15,     4,    22,    48,    31,  5421,\n",
            "         1142,  1235,    10,    14,    11,  4021,   108,  1397,     7,   696,\n",
            "        12741,     6,  9217, 13245,   140,    17,     4,  5388,  1287,     7,\n",
            "            4,     3,  7612,    14,   108,    88,   643,    26,  4945,     7,\n",
            "          103,   108,     7,  1645,    97,    46,  2921,  2858,     4,  7558,\n",
            "           17,     3,     6,     4, 15306,    17,     3,    54,   489,   706,\n",
            "          174,    88,    26,   231,    55,  2837,    75,  2202,    15,    13,\n",
            "           61,    29,     5,    53,    20,    18,    54,   325, 10515,    39,\n",
            "            4, 14548,     6,    26,     3,    23,     4,  5644,     6,    24,\n",
            "          476,     8, 12007,   229,   103,  1251,    47,    25,    44,   351,\n",
            "          141,     8,  1310,     4,    63,   137,   126,     3,     3,    13,\n",
            "            9,     4,    20,    18,    25,     2])\n",
            "tensor([0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test DataLoader**"
      ],
      "metadata": {
        "id": "DD39vgPUZkH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, train_dataset, df, text_column, label_column):\n",
        "        self.train_dataset = train_dataset\n",
        "        self.df = df\n",
        "        \n",
        "        # get texts and labels\n",
        "        self.texts = self.df[text_column]\n",
        "        self.labels = self.df[label_column]\n",
        "        \n",
        "        # utilizing vocabulary created using training set\n",
        "        self.vocab = self.train_dataset.vocab\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[int(index)]\n",
        "        label = self.labels[index]\n",
        "            \n",
        "        # numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
        "        numerialized_text = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numerialized_text += self.vocab.numericalize(text)\n",
        "        numerialized_text.append(self.vocab.stoi[\"<EOS>\"])\n",
        "    \n",
        "        label = [float(label)]\n",
        "        \n",
        "        return torch.tensor(numerialized_text), torch.tensor(label, requires_grad = True) "
      ],
      "metadata": {
        "id": "d2xaA7uSxw8a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TestDataset(\n",
        "    train_dataset = train_dataset, df = test_df, text_column = 'text', label_column = 'label'\n",
        ")\n",
        "\n",
        "print(f'{df.loc[100]}\\n')\n",
        "\n",
        "text, label = test_dataset[100]\n",
        "print(text)\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3SRDnl4tX_d",
        "outputId": "906be642-eb8d-4881-d487-40f3841a74db"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text     neat premise very unrealistic what i learned f...\n",
            "label                                                    0\n",
            "Name: 100, dtype: object\n",
            "\n",
            "tensor([    1,     4,    90,    63,    12,   204,    13,    22,    12,   421,\n",
            "           11,    11,    16, 11182,    15,    12,   233,     5,  1565,  3285,\n",
            "         3705,    12,    93,   143,   334,    19,     4,  2015,     7,   208,\n",
            "         4164, 15913,    12,  3470,    49,     7,    14, 13442,    15,     8,\n",
            "           73,    10,    67,   983,     8,   187,   130,   254,  1332,    38,\n",
            "           26,   399,    52, 12012,   746,     8,    82,    12,   641,    11,\n",
            "            4, 12012,     3,    15, 12012,   151,    79,    11,    58,   157,\n",
            "         2304,   296,    27,  4919,    16,    24,     7,    13,  3878,    15,\n",
            "           13,    22,   412,  1160,    50,     7,  2202,    15,    12,    84,\n",
            "          415,    14,  2304,   492,     4,   594,  1673,     7,   388,  1532,\n",
            "            6,   607,    19,    27,     3,    15,    84,    30,    16,     5,\n",
            "           56,  1477,     3,    15,    32,     5,     3,     6,   130,   688,\n",
            "           15,    56,  2320,     2])\n",
            "tensor([1.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collate Fn of DataLoader"
      ],
      "metadata": {
        "id": "CxWQbmZwZpll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "        \n",
        "    def __call__(self, batch):\n",
        "        source = [item[0] for item in batch] \n",
        "        source = pad_sequence(source, batch_first=True, padding_value = self.pad_idx) \n",
        "        \n",
        "        target = torch.tensor([item[1].item() for item in batch])\n",
        "        return source, target"
      ],
      "metadata": {
        "id": "oPKcjRUezK-L"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset = train_dataset, batch_size = 32, num_workers = 1, shuffle = True, pin_memory = True, drop_last = True,\n",
        "    collate_fn = MyCollate(pad_idx = train_dataset.vocab.stoi[\"<PAD>\"])\n",
        ")"
      ],
      "metadata": {
        "id": "gDYOViZdzLAh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset = test_dataset, batch_size = 64, num_workers = 1, shuffle = True, pin_memory = True, \n",
        "    collate_fn = MyCollate(pad_idx = train_dataset.vocab.stoi[\"<PAD>\"])\n",
        ")"
      ],
      "metadata": {
        "id": "LLbJ9c1Ez9qM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, (texts, labels) in enumerate(train_loader):\n",
        "    print(texts.shape, labels.shape)\n",
        "    if idx >= 4:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6soBhtttYCD",
        "outputId": "824edac8-0e26-4361-fe32-a8fa3244414c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 601]) torch.Size([32])\n",
            "torch.Size([32, 966]) torch.Size([32])\n",
            "torch.Size([32, 614]) torch.Size([32])\n",
            "torch.Size([32, 803]) torch.Size([32])\n",
            "torch.Size([32, 871]) torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for (text, label) in train_loader:\n",
        "    print(f\"{text.shape}\\n{type(text)}\\n{text}\")\n",
        "    print(f\"\\n{label.shape}\\n{type(label)}\\n{label}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdEjcpIkzj78",
        "outputId": "3ec6937a-aba9-4e96-a8ef-71f18ab02857"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 955])\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[   1,  104, 9452,  ...,    0,    0,    0],\n",
            "        [   1,   12,   28,  ...,    0,    0,    0],\n",
            "        [   1,   32,  250,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,   41,  372,  ...,    0,    0,    0],\n",
            "        [   1,   12,  175,  ...,    0,    0,    0],\n",
            "        [   1,   13,   20,  ...,    0,    0,    0]])\n",
            "\n",
            "torch.Size([32])\n",
            "<class 'torch.Tensor'>\n",
            "tensor([1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e7Gncd3fz7Hb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Neural Bag-of-Words Model"
      ],
      "metadata": {
        "id": "XRQ92S-AZ2L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BagOfWords(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_index):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "    \n",
        "    def forward(self, ids):\n",
        "        # ids = [batch size, seq len]\n",
        "        embedded = self.embedding(ids)\n",
        "        # embedded = [batch size, seq len, embedding dim]\n",
        "        pooled = embedded.mean(dim=1)\n",
        "        # pooled = [batch size, embedding dim]\n",
        "        prediction = self.fc(pooled)\n",
        "        # prediction = [batch size, output dim]\n",
        "        return prediction"
      ],
      "metadata": {
        "id": "MbSE14zRmvO7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "vocab_size = train_dataset.vocab.__len__()\n",
        "embedding_dim = 256\n",
        "output_dim = len(set(df['label'].values))\n",
        "pad_index = train_dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "model = BagOfWords(vocab_size, embedding_dim, output_dim, pad_index)"
      ],
      "metadata": {
        "id": "hlOdJ9hCnfyd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc4GGg0Mk8nr",
        "outputId": "e1dd9502-e8eb-48c6-c88d-323de7465765"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 6,400,514 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "pneRFr_Dk8qU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Test the Model"
      ],
      "metadata": {
        "id": "Li9_kAQKaI6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "iVmpihjGaUVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in tqdm(iterator):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        text, label = batch\n",
        "        text, label = text.to(device), label.to(device)\n",
        "                \n",
        "        predictions = model(text)\n",
        "        predictions_values, predictions_idxs = torch.max(predictions, dim=1, keepdim=False)  # [0] to return values, [1] to return indices\n",
        "\n",
        "        predictions_values = torch.round(predictions_values)\n",
        "\n",
        "        loss = criterion(predictions_values.float(), label)\n",
        "        \n",
        "        correction = (predictions_values == label).float()\n",
        "        acc = correction.sum() / len(correction)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
      ],
      "metadata": {
        "id": "g5AaBp9EVsG1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, optimizer, criterion, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(iterator):\n",
        "\n",
        "            text, label = batch\n",
        "            text, label = text.to(device), label.to(device)\n",
        "                    \n",
        "            predictions = model(text)\n",
        "            predictions_values, predictions_idxs = torch.max(predictions, dim=1, keepdim=False)  # [0] to return values, [1] to return indices\n",
        "\n",
        "            predictions_values = torch.round(predictions_values)\n",
        "\n",
        "            loss = criterion(predictions_values.float(), label)\n",
        "            \n",
        "            correction = (predictions_values == label).float()\n",
        "            acc = correction.sum() / len(correction)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
      ],
      "metadata": {
        "id": "nYu45oDXWzE2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "n_epochs = 20\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "valid_losses = []\n",
        "valid_accs = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    print(f'epoch: {epoch+1}')\n",
        "\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "    valid_loss, valid_acc = evaluate(model, test_loader, optimizer, criterion, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    valid_losses.append(valid_loss)\n",
        "    valid_accs.append(valid_acc)\n",
        "    \n",
        "    epoch_train_loss = np.mean(train_loss)\n",
        "    epoch_train_acc = np.mean(train_acc)\n",
        "    epoch_valid_loss = np.mean(valid_loss)\n",
        "    epoch_valid_acc = np.mean(valid_acc)\n",
        "    \n",
        "    if epoch_valid_loss < best_valid_loss:\n",
        "        best_valid_loss = epoch_valid_loss\n",
        "        torch.save(model.state_dict(), 'nbow.pt')\n",
        "    \n",
        "    print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n",
        "    print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}\\n')\n"
      ],
      "metadata": {
        "id": "QSFsj3pqk9UI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e7f298-306e-4eb4-952c-690eee74fd1d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:12<00:00, 100.86it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 68.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.446, valid_acc: 0.496\n",
            "\n",
            "epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 106.14it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 69.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.437, valid_acc: 0.496\n",
            "\n",
            "epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 106.72it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 67.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.464, valid_acc: 0.497\n",
            "\n",
            "epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 106.57it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 67.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.464, valid_acc: 0.497\n",
            "\n",
            "epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 105.01it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 67.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.411, valid_acc: 0.495\n",
            "\n",
            "epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:15<00:00, 80.98it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 67.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.429, valid_acc: 0.496\n",
            "\n",
            "epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 106.54it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 68.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.446, valid_acc: 0.496\n",
            "\n",
            "epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 104.62it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 67.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.411, valid_acc: 0.495\n",
            "\n",
            "epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 106.04it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 67.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.455, valid_acc: 0.497\n",
            "\n",
            "epoch: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 106.93it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 67.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.411, valid_acc: 0.495\n",
            "\n",
            "epoch: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 106.45it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 68.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.437, valid_acc: 0.496\n",
            "\n",
            "epoch: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 107.15it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 66.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.446, valid_acc: 0.496\n",
            "\n",
            "epoch: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 105.50it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 66.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.420, valid_acc: 0.495\n",
            "\n",
            "epoch: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 105.91it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 65.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.402, valid_acc: 0.495\n",
            "\n",
            "epoch: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 106.08it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 66.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.446, valid_acc: 0.496\n",
            "\n",
            "epoch: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 105.89it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 67.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.420, valid_acc: 0.495\n",
            "\n",
            "epoch: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 107.40it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 66.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.429, valid_acc: 0.496\n",
            "\n",
            "epoch: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 106.54it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 67.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.411, valid_acc: 0.495\n",
            "\n",
            "epoch: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 104.91it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 67.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.446, valid_acc: 0.496\n",
            "\n",
            "epoch: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1250/1250 [00:11<00:00, 106.25it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 66.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 55.341, train_acc: 0.501\n",
            "valid_loss: 133.437, valid_acc: 0.496\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "zJw6GpUSaQ-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text, model, device):\n",
        "    ids = train_dataset.vocab.numericalize(text)\n",
        "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
        "    prediction = model(tensor).squeeze(dim=0)\n",
        "    probability = torch.softmax(prediction, dim=-1)\n",
        "    predicted_class = prediction.argmax(dim=-1).item()\n",
        "    predicted_probability = probability[predicted_class].item()\n",
        "    print(f\"{'Negative' if predicted_class == 0 else 'Positive'} | probability score = {predicted_probability:.4f}\")"
      ],
      "metadata": {
        "id": "u0zjXQ5ok9s3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment('This film is terrible', model, device)"
      ],
      "metadata": {
        "id": "t1F95HVbk9vU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24b07dd-70df-487b-9957-ab05a9f966f8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative | probability score = 0.5226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment('This film is great', model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDEnbSysvaxI",
        "outputId": "dd0ab458-66ad-4ec7-975c-9b66c8571639"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive | probability score = 0.5190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9-1o0-wVvazt"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}
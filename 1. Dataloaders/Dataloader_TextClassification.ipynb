{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Clone the GitHub Repository"],"metadata":{"id":"LWl_0H6hO7zE"}},{"cell_type":"code","source":["!git clone https://github.com/mehedihasanbijoy/PyTorch-NLP-Tutorial.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0DzWMMgO1W0","executionInfo":{"status":"ok","timestamp":1667793395025,"user_tz":-360,"elapsed":1486,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}},"outputId":"368bd315-deb7-4f29-807f-733968bad7c5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'PyTorch-NLP-Tutorial'...\n","remote: Enumerating objects: 43, done.\u001b[K\n","remote: Counting objects: 100% (43/43), done.\u001b[K\n","remote: Compressing objects: 100% (38/38), done.\u001b[K\n","remote: Total 43 (delta 13), reused 16 (delta 2), pack-reused 0\u001b[K\n","Unpacking objects: 100% (43/43), done.\n"]}]},{"cell_type":"markdown","source":["## Load the Dataset"],"metadata":{"id":"7EdrVMdOPZLF"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, Dataset"],"metadata":{"id":"dBg3fCnLD3bV","executionInfo":{"status":"ok","timestamp":1667793461455,"user_tz":-360,"elapsed":3484,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/PyTorch-NLP-Tutorial/1. Text Classification/corpus/TweetSentiment.csv')\n","df.dropna(inplace=True)\n","df.reset_index(inplace=True)\n","df.sample(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"rKpcbOYkhhAw","outputId":"0a1f99cf-8cde-451a-b9b6-0b77d3485b8f","executionInfo":{"status":"ok","timestamp":1667793461456,"user_tz":-360,"elapsed":26,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}}},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       index                                               text  \\\n","10369  10661   daddy wanted to buy me shoes, but they were p...   \n","4667    4807    Oh, no need to stop. I could use the ego boost.   \n","\n","                                            cleaned_text sentiment  label  \n","10369  daddy wanted buy shoes but they were prettie e...   neutral    0.0  \n","4667                   need stop could use the ego boost   neutral    0.0  "],"text/html":["\n","  <div id=\"df-dbf54ded-913c-46fa-ac36-7995f47507c0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>text</th>\n","      <th>cleaned_text</th>\n","      <th>sentiment</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>10369</th>\n","      <td>10661</td>\n","      <td>daddy wanted to buy me shoes, but they were p...</td>\n","      <td>daddy wanted buy shoes but they were prettie e...</td>\n","      <td>neutral</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4667</th>\n","      <td>4807</td>\n","      <td>Oh, no need to stop. I could use the ego boost.</td>\n","      <td>need stop could use the ego boost</td>\n","      <td>neutral</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dbf54ded-913c-46fa-ac36-7995f47507c0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-dbf54ded-913c-46fa-ac36-7995f47507c0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-dbf54ded-913c-46fa-ac36-7995f47507c0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["## Build Vocabulary"],"metadata":{"id":"4zn8rsWuPdyE"}},{"cell_type":"code","source":["class Vocabulary: \n","    def __init__(self, freq_threshold=10, max_size=10000):\n","        '''\n","        freq_threshold : the minimum times a word must occur in corpus to be included in vocabulary\n","        max_size : max vocab size\n","        '''\n","        self.freq_threshold = freq_threshold\n","        self.max_size = max_size\n","\n","        self.itos = {0: '<PAD>', 1:'<SOS>', 2:'<EOS>', 3: '<UNK>'}\n","        self.stoi = {k:j for j, k in self.itos.items()} \n","          \n","    \n","    def __len__(self):\n","        return len(self.itos)\n","    \n","\n","    @staticmethod\n","    def tokenizer(text):\n","        return [tok.lower().strip() for tok in text.split(' ')]\n","    \n","    \n","    def build_vocabulary(self, sentence_list):\n","        '''\n","        build the vocabulary: create a dictionary mapping of index to string (itos) and string to index (stoi)\n","        (itos) -> {5:'the', 6:'a', 7:'an'} | (stoi) -> {'the':5, 'a':6, 'an':7}\n","        '''\n","        frequencies = {} \n","        idx = 4  # because 4 tokens already added -> (itos) -> {0: '<PAD>', 1:'<SOS>', 2:'<EOS>', 3: '<UNK>'}\n","        \n","        # calculate the freq of words\n","        for sentence in sentence_list:\n","            for word in self.tokenizer(sentence):\n","                if word not in frequencies.keys():\n","                    frequencies[word] = 1\n","                else:\n","                    frequencies[word] += 1\n","                    \n","                    \n","        # limit vocab by removing low freq words\n","        frequencies = {k:v for k,v in frequencies.items() if v > self.freq_threshold} \n","        \n","        # limit vocab to the max_size specified\n","        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx = 4 for pad, start, end , unk\n","            \n","        # create vocab\n","        for word in frequencies.keys():\n","            self.stoi[word] = idx\n","            self.itos[idx] = word\n","            idx += 1\n","\n"," \n","    def numericalize(self, text):\n","        '''\n","        convert the list of words to a list of corresponding indexes\n","        eg. cat and a dog -> [4, 5, 6, 3]\n","        '''   \n","        tokenized_text = self.tokenizer(text)  # tokenize text \n","        numericalized_text = []\n","\n","        for token in tokenized_text:\n","            if token in self.stoi.keys():\n","                numericalized_text.append(self.stoi[token])\n","            else: # out-of-vocab (OOV) words are represented by UNK token index\n","                numericalized_text.append(self.stoi['<UNK>'])\n","                \n","        return numericalized_text"],"metadata":{"id":"cflIwm7OhhYz","executionInfo":{"status":"ok","timestamp":1667793461456,"user_tz":-360,"elapsed":23,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# create a vocab class with freq_threshold=1 and max_size=100\n","VOCAB = Vocabulary(1, 100)\n","\n","sentence_list = [\n","    'that is a cat', 'that is not a dog', 'a cat and a dog'\n","]\n","\n","# build vocab\n","VOCAB.build_vocabulary(sentence_list)\n","\n","print('index to string: ', VOCAB.itos)\n","print('string to index:', VOCAB.stoi)\n","\n","print(f'\\nnumericalize -> cat and a dog: {VOCAB.numericalize(sentence_list[0])}')\n","print(f\"numericalize -> Hello world: {VOCAB.numericalize('Hello world')}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_rSxvJushhbf","outputId":"e8e20849-b03b-479b-a8c3-583b1863b431","executionInfo":{"status":"ok","timestamp":1667793461457,"user_tz":-360,"elapsed":24,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["index to string:  {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>', 4: 'a', 5: 'that', 6: 'is', 7: 'cat', 8: 'dog'}\n","string to index: {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'a': 4, 'that': 5, 'is': 6, 'cat': 7, 'dog': 8}\n","\n","numericalize -> cat and a dog: [5, 6, 4, 7]\n","numericalize -> Hello world: [3, 3]\n"]}]},{"cell_type":"markdown","source":["## Train Dataset"],"metadata":{"id":"D8JJFocfPp_C"}},{"cell_type":"code","source":["class TrainDataset(Dataset):\n","\n","    def __init__(self, df, text_column, label_column, freq_threshold=5, vocab_size=10000):\n","        self.df = df\n","        \n","        # get texts and labels\n","        self.texts = self.df[text_column]\n","        self.labels = self.df[label_column]\n","        \n","        # build vocabulary\n","        self.vocab = Vocabulary(freq_threshold, vocab_size)\n","        self.vocab.build_vocabulary(self.texts.tolist())\n","\n","\n","    def __len__(self):\n","        return len(self.df)\n","    \n","    \n","    def __getitem__(self, index):\n","        text = self.texts[int(index)]\n","        label = self.labels[index]\n","            \n","        # numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n","        numerialized_text = [self.vocab.stoi[\"<SOS>\"]]\n","        numerialized_text += self.vocab.numericalize(text)\n","        numerialized_text.append(self.vocab.stoi[\"<EOS>\"])\n","    \n","        label = [int(label)]\n","        \n","        return torch.tensor(numerialized_text), torch.tensor(label) "],"metadata":{"id":"E-lVzYy0zOxh","executionInfo":{"status":"ok","timestamp":1667793461457,"user_tz":-360,"elapsed":21,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["train_dataset = TrainDataset(\n","    df = df, text_column = 'cleaned_text', label_column = 'label', \n","    freq_threshold = 5, vocab_size = 10000\n",")\n","\n","print(f'{df.loc[1]}\\n')\n","\n","text, target = train_dataset[1]\n","print(text)\n","print(target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qSYOZb0FzO0o","outputId":"9c7d4cd4-bd36-4982-93db-1923a6e4deb0","executionInfo":{"status":"ok","timestamp":1667793461458,"user_tz":-360,"elapsed":21,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["index                                                        1\n","text             Sooo SAD I will miss you here in San Diego!!!\n","cleaned_text             sooo sad will miss you here san diego\n","sentiment                                             negative\n","label                                                      1.0\n","Name: 1, dtype: object\n","\n","tensor([   1,  376,   86,   42,   66,    6,   64, 1328, 2006,    2])\n","tensor([1])\n"]}]},{"cell_type":"markdown","source":["## Test Dataset"],"metadata":{"id":"n7BrQbpHPttU"}},{"cell_type":"code","source":["class TestDataset(Dataset):\n","\n","    def __init__(self, train_dataset, df, text_column, label_column):\n","        self.train_dataset = train_dataset\n","        self.df = df\n","        \n","        # get texts and labels\n","        self.texts = self.df[text_column]\n","        self.labels = self.df[label_column]\n","        \n","        # utilizing vocabulary created using training set\n","        self.vocab = self.train_dataset.vocab\n","\n","\n","    def __len__(self):\n","        return len(self.df)\n","    \n","    \n","    def __getitem__(self, index):\n","        text = self.texts[int(index)]\n","        label = self.labels[index]\n","            \n","        # numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n","        numerialized_text = [self.vocab.stoi[\"<SOS>\"]]\n","        numerialized_text += self.vocab.numericalize(text)\n","        numerialized_text.append(self.vocab.stoi[\"<EOS>\"])\n","    \n","        label = [int(label)]\n","        \n","        return torch.tensor(numerialized_text), torch.tensor(label) "],"metadata":{"id":"wOqav2hvMNGE","executionInfo":{"status":"ok","timestamp":1667793461982,"user_tz":-360,"elapsed":544,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["test_dataset = TestDataset(\n","    train_dataset = train_dataset, df = df, text_column = 'cleaned_text', label_column = 'label'\n",")\n","\n","print(f'{df.loc[100]}\\n')\n","\n","text, target = test_dataset[100]\n","print(text)\n","print(target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9q4qCsXIMNI0","outputId":"9eb5e73d-2dd6-4584-9d3d-301288a0d1cd","executionInfo":{"status":"ok","timestamp":1667793461982,"user_tz":-360,"elapsed":12,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["index                                                         106\n","text             cool i wear black most of the time when i go out\n","cleaned_text               cool wear black most the time when out\n","sentiment                                                 neutral\n","label                                                         0.0\n","Name: 100, dtype: object\n","\n","tensor([  1, 153, 725, 656, 298,   4,  39,  60,  21,   2])\n","tensor([0])\n"]}]},{"cell_type":"markdown","source":["## Collate Function"],"metadata":{"id":"lrRJjtgMPyEF"}},{"cell_type":"code","source":["class MyCollate:\n","    def __init__(self, pad_idx):\n","        self.pad_idx = pad_idx\n","        \n","    def __call__(self, batch):\n","        source = [item[0] for item in batch] \n","        source = pad_sequence(source, batch_first=False, padding_value = self.pad_idx) \n","        \n","        target = torch.tensor([item[1].item() for item in batch])\n","        return source, target"],"metadata":{"id":"prvOa92VzO21","executionInfo":{"status":"ok","timestamp":1667793461983,"user_tz":-360,"elapsed":11,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## Train DataLoader"],"metadata":{"id":"6whk4KABP1jW"}},{"cell_type":"code","source":["train_loader = DataLoader(\n","    dataset = train_dataset, batch_size = 32, num_workers = 1, shuffle = True, pin_memory = True, drop_last = True,\n","    collate_fn = MyCollate(pad_idx = train_dataset.vocab.stoi[\"<PAD>\"])\n",")"],"metadata":{"id":"Oy0RjI-GzO8j","executionInfo":{"status":"ok","timestamp":1667793461984,"user_tz":-360,"elapsed":12,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["for idx, (texts, labels) in enumerate(train_loader):\n","    print(texts.shape, labels.shape)\n","    if idx >= 4:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJkJ5IlGmu-p","outputId":"484eabd5-9376-4004-f859-9c5c652d86bb","executionInfo":{"status":"ok","timestamp":1667793461985,"user_tz":-360,"elapsed":12,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([21, 32]) torch.Size([32])\n","torch.Size([23, 32]) torch.Size([32])\n","torch.Size([23, 32]) torch.Size([32])\n","torch.Size([23, 32]) torch.Size([32])\n","torch.Size([23, 32]) torch.Size([32])\n"]}]},{"cell_type":"markdown","source":["## Test DataLoader"],"metadata":{"id":"ONzgBoG9P4X0"}},{"cell_type":"code","source":["test_loader = DataLoader(\n","    dataset = test_dataset, batch_size = 64, num_workers = 1, shuffle = True, pin_memory = True, \n","    collate_fn = MyCollate(pad_idx = train_dataset.vocab.stoi[\"<PAD>\"])\n",")"],"metadata":{"id":"xY-OYHeANQY8","executionInfo":{"status":"ok","timestamp":1667793461985,"user_tz":-360,"elapsed":10,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["for idx, (texts, labels) in enumerate(train_loader):\n","    print(texts.shape, labels.shape)\n","    if idx >= 4:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rI6dUUzMzPEw","outputId":"2f788a9c-84ad-4bdd-e5d7-0f59efcb317f","executionInfo":{"status":"ok","timestamp":1667793461986,"user_tz":-360,"elapsed":10,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([23, 32]) torch.Size([32])\n","torch.Size([19, 32]) torch.Size([32])\n","torch.Size([20, 32]) torch.Size([32])\n","torch.Size([20, 32]) torch.Size([32])\n","torch.Size([25, 32]) torch.Size([32])\n"]}]},{"cell_type":"markdown","source":["## References"],"metadata":{"id":"rkVx5x5RQBQb"}},{"cell_type":"code","source":["# https://towardsdatascience.com/how-to-use-datasets-and-dataloader-in-pytorch-for-custom-text-data-270eed7f7c00\n","# https://towardsdatascience.com/custom-datasets-in-pytorch-part-2-text-machine-translation-71c41a3e994e"],"metadata":{"id":"8wmC3qGPzPaS","executionInfo":{"status":"ok","timestamp":1667793461986,"user_tz":-360,"elapsed":8,"user":{"displayName":"Mehedi Hasan Bijoy 1721937042","userId":"13632457997826634348"}}},"execution_count":16,"outputs":[]}]}
